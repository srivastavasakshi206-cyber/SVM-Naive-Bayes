{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "ANS . Information Gain is a key concept in the construction of Decision Trees. Here's a breakdown:\n",
        "\n",
        "At its core, Information Gain is a measure used in decision tree algorithms to determine the effectiveness of splitting a dataset based on an attribute. It quantifies how much the uncertainty (entropy) about the target variable decreases after splitting the data based on a particular feature.\n",
        "\n",
        "Imagine you have a mixed bag of fruits, and you want to classify them (e.g., into 'apple' or 'orange'). Initially, there's a certain level of disorder or uncertainty. If you pick an attribute like 'color' (red, green, orange), and splitting the fruits by color helps you group most of the apples together and most of the oranges together, then that 'color' attribute has provided a good 'Information Gain' because it reduced the disorder.\n",
        "\n",
        "Decision Trees are built by recursively splitting the dataset into subsets. At each node of the tree, the algorithm needs to decide which attribute to use for the split. This is where Information Gain comes in:\n",
        "\n",
        "Calculate Entropy: For each potential split, the algorithm first calculates the entropy of the parent node (before the split) and the entropy of the child nodes (after the split). Entropy is a measure of impurity or randomness in a set of data. A higher entropy means more disorder.\n",
        "\n",
        "Calculate Information Gain: Information Gain is then calculated as: Information Gain = Entropy(Parent) - Weighted Average(Entropy(Children)) The weighted average is used because different child nodes might contain different numbers of samples.\n",
        "\n",
        "Select Best Split: The attribute that yields the highest Information Gain is chosen as the best split for that node. This is because the attribute with the highest Information Gain is the one that best separates the data into purer subsets, meaning it most effectively reduces the uncertainty about the target variable.\n",
        "\n",
        "Repeat: This process is repeated recursively for each child node until a stopping criterion is met (e.g., all nodes are pure, or the tree reaches a certain depth)"
      ],
      "metadata": {
        "id": "0-cipbltd-NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oPLGDOx5d-Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.  What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "ANS.  Both Gini Impurity and Entropy are commonly used metrics in decision tree algorithms to measure the 'impurity' or 'randomness' of a set of data. The goal of a decision tree is to minimize this impurity at each split. While they serve the same purpose, they have some key differences:\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "Definition: Gini Impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the distribution of labels in the subset. A Gini Impurity of 0 means all elements belong to a single class (perfect purity).\n",
        "Formula: For a node with 'c' classes, Gini = 1 - Σ (p_i)^2, where p_i is the proportion of samples belonging to class i.\n",
        "Strengths:\n",
        "Computationally Less Intensive: It involves squaring probabilities, which is less computationally expensive than the logarithmic calculations in Entropy.\n",
        "Favors Larger Partitions: Tends to isolate the most frequent class in its own branch of the tree.\n",
        "Binary Splits: Often preferred for binary splits.\n",
        "Weaknesses:\n",
        "Less Sensitive to Changes: Can be less sensitive to small changes in class probabilities compared to Entropy.\n",
        "Bias towards larger classes: It can sometimes be biased towards creating splits that result in larger partitions that are relatively pure for one class, even if it means smaller, less pure partitions for other classes.\n",
        "Use Case: Often the default choice in many implementations (like scikit-learn's DecisionTreeClassifier) due to its computational efficiency. Suitable when you want faster training or when the exact amount of 'information' isn't as critical as finding a good split quickly.\n",
        "Entropy:\n",
        "\n",
        "Definition: Entropy is a measure of the unpredictability or impurity of a set of data. In the context of decision trees, it quantifies the amount of 'information' needed to classify a new sample. Higher entropy means more disorder and uncertainty.\n",
        "Formula: For a node with 'c' classes, Entropy = - Σ p_i * log2(p_i), where p_i is the proportion of samples belonging to class i.\n",
        "Strengths:\n",
        "More Sensitive to Changes: It is more sensitive to changes in class distribution and aims to reduce uncertainty more broadly across all classes.\n",
        "Balances Splits: Tends to produce more balanced trees, trying to make all child nodes as pure as possible.\n",
        "Information Gain: Directly related to the concept of Information Gain (as discussed previously), making it intuitively easier to understand as a measure of information content.\n",
        "Weaknesses:\n",
        "Computationally More Intensive: Involves logarithmic calculations, which can be slower than Gini Impurity for very large datasets.\n",
        "Use Case: Preferred when you want to maximize the information gain at each split and potentially create a more balanced tree. It's often used in research or when a deeper understanding of the information content of splits is desired.\n",
        "Key Similarities:\n",
        "\n",
        "Both are measures of impurity, with a value of 0 indicating perfect purity.\n",
        "Both aim to find splits that minimize impurity and maximize the homogeneity of the child nodes.\n",
        "Both will generally lead to similar decision trees, especially for large datasets, as they both aim for the same outcome: purest possible splits.\n",
        "In essence:\n",
        "\n",
        "Gini Impurity is often favored for its computational speed and simplicity, especially for binary classification problems.\n",
        "Entropy is conceptually rooted in information theory and can be more sensitive to complex class distributions, potentially leading to more balanced trees, but at a slightly higher computational cost.\n",
        "The choice between them often comes down to performance considerations and specific problem characteristics, but in many practical scenarios, their performance difference is minimal.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq9xc5BZd-HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UxjFb6Qyd-Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.  What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "ANS.  Pre-pruning, also known as early stopping, is a technique used in decision tree algorithms to prevent overfitting by stopping the tree construction process prematurely. Instead of building a full decision tree and then pruning it back (which is called post-pruning), pre-pruning involves setting criteria during the tree's growth that, when met, stop further splitting of a node.\n",
        "\n",
        "Here's a breakdown of what pre-pruning is and how it works:\n",
        "\n",
        "What is Overfitting in Decision Trees?\n",
        "\n",
        "Decision trees can easily become overly complex, capturing noise and specific patterns in the training data that do not generalize well to unseen data. This phenomenon is called overfitting. An overfitted tree might have many branches and nodes, making it very accurate on the training set but poor on new, unclassified examples.\n",
        "\n",
        "How Pre-Pruning Works:\n",
        "\n",
        "During the recursive process of building a decision tree, pre-pruning continuously checks for certain conditions. If any of these conditions are met at a node, the splitting process for that node is halted, and it becomes a leaf node. Common criteria used for pre-pruning include:\n",
        "\n",
        "Maximum Depth: The tree is not allowed to grow beyond a certain number of levels. Once a node reaches this maximum depth, it cannot be split further.\n",
        "Minimum Samples per Split: A node must contain a minimum number of samples (data points) to be considered for a split. If a node has fewer samples than this threshold, it won't be split.\n",
        "Minimum Samples per Leaf Node: A split is only allowed if each resulting child node (leaf) would contain at least a specified minimum number of samples. This prevents the creation of leaves with very few data points, which are often highly specific to the training data.\n",
        "Maximum Number of Leaf Nodes: The total number of leaf nodes in the tree is limited. Once this limit is reached, no further splits are performed.\n",
        "Impurity Decrease (or Information Gain Threshold): A split is only performed if it results in a significant decrease in impurity (or a significant increase in information gain). If the improvement achieved by a potential split is below a certain threshold, the split is not made.\n",
        "Advantages of Pre-Pruning:\n",
        "\n",
        "Reduces Overfitting: By stopping the growth of complex trees, it helps prevent the model from memorizing the training data.\n",
        "Faster Training: Since the tree is not grown to its full potential, the training process can be significantly faster.\n",
        "Simpler Models: The resulting trees are typically smaller and easier to interpret.\n",
        "Less Computational Cost: It avoids the need to build a full tree and then perform a separate pruning step.\n",
        "Disadvantages of Pre-Pruning:\n",
        "\n",
        "Greedy Approach: It makes decisions about stopping a split based on local information at each node. It might stop splitting too early, missing potentially beneficial splits further down the tree that would have led to a better overall model.\n",
        "Difficulty in Setting Thresholds: Determining the optimal thresholds for the pre-pruning criteria (e.g., max depth, min samples) can be challenging and often requires cross-validation or domain knowledge.\n",
        "May Underfit: If the pruning criteria are too strict, the tree might be too simple and fail to capture important patterns in the data, leading to underfitting."
      ],
      "metadata": {
        "id": "NqGxF-wVd-CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wA4EE6m3d9_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.  Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances"
      ],
      "metadata": {
        "id": "bHRGEWnkd99B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5b57b9b",
        "outputId": "f2633bd2-d5cf-4276-949b-18335b7e3df2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Feature names: {feature_names}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Number of samples: 150\n",
            "Number of features: 4\n",
            "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db2d50a",
        "outputId": "ffeea23c-bc53-4546-e2d7-9f53b91f6d8d"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 105 samples\n",
            "Testing set size: 45 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d30161f9",
        "outputId": "f2766438-e34d-43e6-a650-2ac6b218de94"
      },
      "source": [
        "dtree_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "dtree_gini.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier trained successfully using Gini impurity.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier trained successfully using Gini impurity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8351213b",
        "outputId": "48c9824f-088b-4aa7-e480-e18ad77f017b"
      },
      "source": [
        "feature_importances = dtree_gini.feature_importances_\n",
        "\n",
        "importance_df = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "print(\"\\nFeature Importances (Gini Impurity):\")\n",
        "print(importance_df.sort_values(ascending=False))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importances (Gini Impurity):\n",
            "petal length (cm)    0.893264\n",
            "petal width (cm)     0.087626\n",
            "sepal width (cm)     0.019110\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fHJXqt0Id965"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.  What is a Support Vector Machine (SVM)?\n",
        "\n",
        "ANS.  A Support Vector Machine (SVM) is a powerful and versatile machine learning algorithm used for classification, regression, and outlier detection. It's particularly effective in high-dimensional spaces and cases where the number of dimensions is greater than the number of samples.\n",
        "\n",
        "Here's a breakdown of what an SVM is and its core concepts:\n",
        "\n",
        "Core Idea: Finding the Optimal Hyperplane\n",
        "\n",
        "At its heart, an SVM aims to find the optimal hyperplane that best separates different classes in a dataset. In a 2D space, this hyperplane is a line; in 3D, it's a plane; and in higher dimensions, it's referred to as a hyperplane.\n",
        "\n",
        "The 'optimal' hyperplane is defined by two key characteristics:\n",
        "\n",
        "Separation: It maximizes the margin between the closest data points of different classes.\n",
        "Margin Maximization: It finds the hyperplane that has the largest distance to the nearest training data point of any class. These closest points are called support vectors.\n",
        "Key Concepts:\n",
        "\n",
        "Hyperplane: A decision boundary that separates data points of different classes.\n",
        "Support Vectors: These are the data points that are closest to the hyperplane. They are the critical elements in defining the decision boundary and the margin. If you remove the support vectors, the position of the hyperplane would change.\n",
        "Margin: The distance between the hyperplane and the nearest data points (support vectors) from each class. SVMs aim to maximize this margin, as a larger margin generally leads to better generalization and lower classification error on unseen data.\n",
        "Linear vs. Non-linear SVMs:\n",
        "\n",
        "Linear SVM: When data points can be perfectly separated by a straight line (or hyperplane) in their original feature space, it's called a linearly separable dataset, and a Linear SVM can be used.\n",
        "\n",
        "Non-linear SVM (Kernel Trick): Often, data points are not linearly separable. In such cases, SVMs employ a technique called the kernel trick. The kernel trick implicitly maps the input features into a higher-dimensional feature space where they might become linearly separable. Without actually performing the computations in this high-dimensional space, the kernel function calculates the dot product of the transformed vectors, making the process computationally feasible. Common kernel functions include:\n",
        "\n",
        "Polynomial Kernel: Transforms data using polynomial features.\n",
        "Radial Basis Function (RBF) / Gaussian Kernel: Maps data to an infinite-dimensional space, effective for non-linear boundaries.\n",
        "Sigmoid Kernel: Based on the hyperbolic tangent function.\n",
        "Soft Margin vs. Hard Margin:\n",
        "\n",
        "Hard Margin SVM: This type of SVM seeks to find a hyperplane that perfectly separates the classes without any misclassifications. This works well only if the data is perfectly linearly separable and free of outliers. If outliers are present, a hard margin might lead to a hyperplane that doesn't generalize well.\n",
        "Soft Margin SVM: In most real-world scenarios, perfect separation isn't possible or desirable due to noise or overlapping classes. Soft margin SVM allows for some misclassifications or points to fall within the margin. It introduces a regularization parameter (often denoted as C) that controls the trade-off between maximizing the margin and minimizing the classification errors.\n",
        "A small C value emphasizes a wider margin, tolerating more misclassifications.\n",
        "A large C value emphasizes minimizing misclassifications, potentially leading to a narrower margin and a more complex model.\n",
        "Advantages of SVMs:\n",
        "\n",
        "Effective in high-dimensional spaces: Works well even when the number of features is greater than the number of samples.\n",
        "Effective in cases with clear margin of separation: Excels when classes are well-separated.\n",
        "Memory efficient: Uses a subset of training points in the decision function (the support vectors).\n",
        "Versatile with kernel functions: Can be adapted to various data types and complex decision boundaries.\n",
        "Disadvantages of SVMs:\n",
        "\n",
        "Not suitable for large datasets: Training time can be long for very large datasets.\n",
        "Less effective on noisy datasets: Performance can degrade if the dataset has a lot of noise, especially with a hard margin.\n",
        "Sensitive to feature scaling: Features should be scaled before training.\n",
        "Choosing the right kernel and parameters: Selecting the optimal kernel function and hyperparameters (like C and kernel-specific parameters) can be challenging and often requires cross-validation."
      ],
      "metadata": {
        "id": "zDfNrOUId94T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EEe0axaPd91r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.  What is the Kernel Trick in SVM?\n",
        "\n",
        "ANS.  The Kernel Trick is a fundamental and ingenious technique used in Support Vector Machines (SVMs) that allows them to handle non-linearly separable data effectively, without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "1. The Problem: Non-linearly Separable Data\n",
        "\n",
        "Many real-world datasets are not linearly separable. This means you cannot draw a single straight line (or hyperplane in higher dimensions) to perfectly divide the different classes in their original feature space. If you try to use a linear SVM on such data, it will perform poorly.\n",
        "\n",
        "2. The Idea: Mapping to a Higher Dimension\n",
        "\n",
        "The core idea to solve this non-linearity is to map the data from its original, lower-dimensional space into a higher-dimensional feature space where it might become linearly separable. Once linearly separable in this new space, a standard linear SVM can be used to find an optimal hyperplane.\n",
        "\n",
        "3. The Challenge: Computational Cost\n",
        "\n",
        "Explicitly transforming data into a very high (or even infinite) dimensional space can be computationally extremely expensive or even impossible. This is where the Kernel Trick comes in.\n",
        "\n",
        "4. The Solution: The Kernel Trick\n",
        "\n",
        "The Kernel Trick avoids the need to explicitly compute the coordinates of the data points in the higher-dimensional space. Instead, it uses a kernel function (or simply, a kernel) that calculates the dot product of the data points as if they were already in the higher-dimensional space. This calculation is often much less computationally intensive than the actual transformation.\n"
      ],
      "metadata": {
        "id": "xfKAb-RGd9zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E6RiPn2Pd9lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.  Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "V9jDXJ5Wg2SK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b72057a",
        "outputId": "7fd64bf0-5422-48ae-c6fe-93aac3cd28e5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "print(\"Wine dataset loaded successfully.\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Feature names: {feature_names}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wine dataset loaded successfully.\n",
            "Number of samples: 178\n",
            "Number of features: 13\n",
            "Feature names: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46b16b4a",
        "outputId": "94b464fa-0309-45e6-d44b-4dca187f75d9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 124 samples\n",
            "Testing set size: 54 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b02c4f04",
        "outputId": "296666a4-a7ae-4476-bbd4-f6a1f2e7333d"
      },
      "source": [
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Linear SVM Accuracy: {accuracy_linear:.4f}\")\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"RBF SVM Accuracy: {accuracy_rbf:.4f}\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(f\"Linear SVM performed better with an accuracy of {accuracy_linear:.4f}\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(f\"RBF SVM performed better with an accuracy of {accuracy_rbf:.4f}\")\n",
        "else:\n",
        "    print(f\"Both Linear and RBF SVMs performed equally with an accuracy of {accuracy_linear:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9815\n",
            "RBF SVM Accuracy: 0.7593\n",
            "\n",
            "--- Comparison ---\n",
            "Linear SVM performed better with an accuracy of 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ldk-CmzSg3D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.  What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "ANS.  The Naïve Bayes classifier is a family of probabilistic algorithms based on Bayes' Theorem with a strong (naïve) independence assumption between the features. It's a simple yet powerful classification technique, particularly popular for tasks like text classification (e.g., spam detection, sentiment analysis) due to its speed and efficiency.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "What is Naïve Bayes?\n",
        "\n",
        "At its core, a Naïve Bayes classifier predicts the probability that a given data point belongs to a particular class, given the observed features."
      ],
      "metadata": {
        "id": "Q0fN5Us8haPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GdbYc0vShyq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "ANS.  1. Gaussian Naïve Bayes\n",
        "Feature Type: Primarily used for continuous features.\n",
        "Assumption: Assumes that the continuous values associated with each class are distributed according to a Gaussian (Normal) distribution.\n",
        "How it works: When calculating the likelihood of a feature value given a class, it uses the probability density function (PDF) of the Gaussian distribution. For each class, it calculates the mean and standard deviation of each feature from the training data. These parameters are then used to compute the probability of a new instance's feature value belonging to that class.\n",
        "Use Cases: Datasets where features are real-valued, such as sensor readings, physical measurements (e.g., height, weight), or any data that can be reasonably modeled by a normal distribution (or approximated as such).\n",
        "2. Multinomial Naïve Bayes\n",
        "Feature Type: Designed for discrete counts, typically representing counts of occurrences. It's often used with features that represent frequencies or proportions.\n",
        "Assumption: Assumes that the features are generated from a multinomial distribution.\n",
        "How it works: It counts how often each feature value (e.g., each word) appears in instances of each class. When predicting, it estimates the probability of observing a particular count for a feature given a class. This is essentially suited for data where the features are integer counts (e.g., how many times a word appears in a document).\n",
        "Use Cases: Most commonly used in Natural Language Processing (NLP) for text classification tasks, such as spam detection, document classification, and sentiment analysis. Features are typically word counts or TF-IDF values.\n",
        "3. Bernoulli Naïve Bayes\n",
        "Feature Type: Designed for binary or boolean features.\n",
        "Assumption: Assumes that the features are generated from a Bernoulli distribution. This means each feature is treated as an independent binary variable (it either occurs or it doesn't).\n",
        "How it works: It explicitly penalizes the non-occurrence of a feature that is typically present in a class. For each class, it learns the probability of each feature being present (having a non-zero value) in an instance of that class. When predicting, it considers both the presence and absence of features.\n",
        "Use Cases: Text classification where features are boolean indicators of word presence (is a word present in the document, yes/no), rather than count. For example, if you only care if a specific keyword appears in an email, not how many times it appears."
      ],
      "metadata": {
        "id": "5L87fYT1hzPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n40H5JNxiM14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.  Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "\n",
        "ANS.  "
      ],
      "metadata": {
        "id": "ER65r5aKiNfJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcefda73",
        "outputId": "92eb1dd3-8a97-487c-dea1-85f37b8855e7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "print(\"Breast Cancer dataset loaded successfully.\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Feature names: {', '.join(feature_names)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast Cancer dataset loaded successfully.\n",
            "Number of samples: 569\n",
            "Number of features: 30\n",
            "Feature names: mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, worst fractal dimension\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81222a65",
        "outputId": "82ffa2f0-3f2b-4f03-91ed-9c486bf82f8a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 398 samples\n",
            "Testing set size: 171 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e7e987",
        "outputId": "af34cebd-2826-4425-fc18-dc69480ac325"
      },
      "source": [
        "gnb_classifier = GaussianNB()\n",
        "\n",
        "gnb_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes classifier trained successfully.\")\n",
        "\n",
        "y_pred = gnb_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy of Gaussian Naïve Bayes classifier: {accuracy:.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes classifier trained successfully.\n",
            "\n",
            "Accuracy of Gaussian Naïve Bayes classifier: 0.9415\n"
          ]
        }
      ]
    }
  ]
}